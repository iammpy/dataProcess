{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e9b805",
   "metadata": {},
   "source": [
    "### 初始化全局变量，导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "610ff416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from model import call_huoshan,call_openai\n",
    "import pandas as pd\n",
    "if \"__file__\" in globals():\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "raw_data_path= os.path.join(\"raw_data\")\n",
    "scienceQA_path = os.path.join(raw_data_path, \"ScienceQA\")\n",
    "sciKnowEval_path = os.path.join(raw_data_path, \"SciKnowEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05edc32a",
   "metadata": {},
   "source": [
    "### 查看读取sciknoweval数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3f7db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciKnowEval_path_list=[]\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_biology_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_chemistry_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_material_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_physics_test.jsonl\"))  \n",
    "\n",
    "all_dfs = []\n",
    "for file_path in sciKnowEval_path_list:\n",
    "    temp_df = pd.read_json(file_path, lines=True) # 添加 lines=True\n",
    "    all_dfs.append(temp_df)\n",
    "\n",
    "sciKnowEval_data = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06e4b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Temperature stability during the measurement', 'Sample preparation technique', 'Intensity of the applied magnetic field', 'The waiting time before applying the weak magnetic field', 'The brand of the commercial SQUID magnetometer'], 'label': ['A', 'B', 'C', 'D']}\n"
     ]
    }
   ],
   "source": [
    "print(sciKnowEval_data.iloc[52100].choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585b6eb",
   "metadata": {},
   "source": [
    "#### 根据一个问题，以及不同的文件类型，构建传给模型的最终prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223bd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def process_choices(choices):\n",
    "    # 传入是\"text\": [\"15.5 - 17.5%\", \"15 - 17%\", \"14 - 16%\", \"16 - 18%\"], \"label\": [\"A\", \"B\", \"C\", \"D\"]\n",
    "    # 返回的是 \"A: 15.5 - 17.5%, B: 15 - 17%, C: 14 - 16%, D: 16 - 18%\"\n",
    "    texts = choices[\"text\"]\n",
    "    labels = choices[\"label\"]\n",
    "    if len(texts) != len(labels):\n",
    "        raise ValueError(\"Choices and labels must have the same length.\")\n",
    "    formatted_choices_list=[\n",
    "        [f\"({label}) {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "        [f\"{label}: {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "        [f\"{label}. {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "        [f\"{label} - {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "        [f\"{label}) {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "    ]\n",
    "    formatted_choices = random.choice(formatted_choices_list)\n",
    "    return \"\".join(formatted_choices)\n",
    "\n",
    "# test= {\n",
    "#     \"text\": [\"15.5 - 17.5%\", \"15 - 17%\", \"14 - 16%\", \"16 - 18%\"], \n",
    "#     \"label\": [\"A\", \"B\", \"C\", \"D\"]\n",
    "# }\n",
    "# print(process_choices(test))\n",
    "\n",
    "def sciKnowEval_build_prompt(\n",
    "    row,\n",
    "    require_range=False              \n",
    "                             ):\n",
    "    # prompt = row[\"prompt\"][\"default\"]\n",
    "    task_type= row[\"type\"]\n",
    "    subtask=row[\"details\"][\"subtask\"]\n",
    "    qusetion = row[\"question\"]\n",
    "    choices = row.get(\"choices\", None)\n",
    "    true_or_false_prompt=[\n",
    "        \"Determine the correctness of this statement, write the correct answer inside a \\\\boxed{} at the end. The wrapped answer will be \\\"Yes\\\" or \\\"No\\\". \",\n",
    "        \"Assess the correctness of the following statement. Your conclusion, which must be either \\\"Yes\\\" or \\\"No\\\", should be placed in \\\\boxed{} at the end. \",\n",
    "        \"Evaluate the truthfulness of the statement. Your final answer should be either \\\"Yes\\\" or \\\"No\\\", enclosed in \\\\boxed{} at the end. \",\n",
    "    ]\n",
    "    choices_prompt=[\n",
    "        \"Answer the question, write the correct answer choice inside a \\\\boxed{} at the end. \",\n",
    "        \"Your final response should conclude with the correct answer choice wrapped in a \\\\boxed{}. \",\n",
    "        \"Please provide the answer to the question, and ensure that your final response includes the correct answer choice wrapped in a \\\\boxed{}. \",\n",
    "    ]\n",
    "    \n",
    "    #task_type的类型有：\n",
    "    #     \"true_or_false\"\n",
    "    #   \n",
    "    #  \"mcq-2-choices\"\n",
    "    # \"open-ended-qa\"\n",
    "\n",
    "    random_index = random.randint(0, 1)\n",
    "    if task_type == \"true_or_false\" :\n",
    "        base_prompt = \"You will be presented with a hypothesis or conjecture. Based on the information provided in a text excerpt or your general knowledge, determine if the hypothesis is true (yes) or false (no). \"\n",
    "        prompt = random.choice(true_or_false_prompt) + \"\\n\\n\"\n",
    "        format_instruction=prompt\n",
    "        problem_list=[\n",
    "            f\"Statement: {qusetion}\\n\\n\",\n",
    "            f\"Hypothesis: {qusetion}\\n\\n\",\n",
    "            f\"Question: {qusetion}\\n\\n\",\n",
    "        ]\n",
    "        problem =base_prompt+ random.choice(problem_list)\n",
    "        if random_index == 0:\n",
    "            prompt= prompt+ problem\n",
    "        else:\n",
    "            prompt = problem + prompt\n",
    "    \n",
    "    elif task_type == \"mcq-4-choices\" or task_type == \"mcq-2-choices\":\n",
    "        \n",
    "        prompt = random.choice(choices_prompt) + \"\\n\\n\"\n",
    "        format_instruction=prompt\n",
    "        if choices is None:\n",
    "            raise ValueError(\"Choices must be provided for mcq-4-choices task type.\")\n",
    "        formatted_choices = process_choices(choices)\n",
    "        problem_list=[\n",
    "            # f\"Question: {qusetion}\",\n",
    "            f\"{qusetion}\",\n",
    "        ]\n",
    "        problem = random.choice(problem_list)\n",
    "        choice_list=[\n",
    "            f\"\\n\\n{formatted_choices}\\n\\n\",\n",
    "        ]\n",
    "        problem += random.choice(choice_list)\n",
    "        if random_index == 0:\n",
    "            prompt = prompt + problem\n",
    "        else:\n",
    "            prompt = problem + prompt\n",
    "    elif task_type == \"open-ended-qa\":\n",
    "        prompt = row[\"prompt\"][\"default\"]+ \"\\n\\n\"\n",
    "        # if subtask ==  \"crystal_structure_and_composition_analysis\":\n",
    "        #     prompt=\"Based on the provided crystallographic data, determine the material's properties as requested in the question and list them clearly.\"\n",
    "        format_instruction=prompt\n",
    "        prompt+= f\"Question: {qusetion}\"\n",
    "\n",
    "    elif task_type == \"filling\":\n",
    "        prompt = row[\"prompt\"][\"default\"]+ \"\\n\\n\"\n",
    "        if require_range:\n",
    "            prompt += \"Please provide the bandgap in a range format, such as 'x - y'.\\n\\n\"\n",
    "        format_instruction=prompt\n",
    "        prompt += f\"Question: {qusetion}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "    return prompt,format_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1791127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD5 of 'Hello, World!': 65a8e27d8879283831b664bd8b7f0ad4\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_md5(input_string):\n",
    "    # 创建一个 md5 hash 对象\n",
    "    md5_hash = hashlib.md5()\n",
    "    \n",
    "    # 将输入的字符串转换为字节串（因为 hashlib 需要字节类型的数据）\n",
    "    input_bytes = input_string.encode('utf-8')\n",
    "    \n",
    "    # 更新哈希对象\n",
    "    md5_hash.update(input_bytes)\n",
    "    \n",
    "    # 获取哈希值的十六进制表示\n",
    "    md5_digest = md5_hash.hexdigest()\n",
    "    \n",
    "    return md5_digest \n",
    "\n",
    "# 示例使用\n",
    "input_string = \"Hello, World!\"\n",
    "md5_result = generate_md5(input_string)\n",
    "print(f\"MD5 of '{input_string}': {md5_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a073d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对 Li2VFe(P2O7)2 进行分步查询...\n"
     ]
    }
   ],
   "source": [
    "from mp_api.client import MPRester\n",
    "import os\n",
    "\n",
    "API_KEY = \"bSpFQg1jCJpFG4CARe0NiSUyXKke56OF\"  # <--- 在这里替换成您的密钥\n",
    "formula_to_search = \"Li2VFe(P2O7)2\"\n",
    "\n",
    "print(f\"开始对 {formula_to_search} 进行分步查询...\")\n",
    "def call_MP(formula_to_search):\n",
    "    try:\n",
    "        with MPRester(api_key=API_KEY) as mpr:\n",
    "\n",
    "            # --- 步骤 1: 查询热力学性质，找到最稳定的材料 ID ---\n",
    "            # 我们先从热力学端点入手，因为稳定性是首要关心的\n",
    "            print(\"\\n步骤 1: 查询热力学性质以确定最稳定的结构...\")\n",
    "            thermo_docs = mpr.materials.thermo.search(\n",
    "                formula=formula_to_search,\n",
    "                fields=[\"material_id\", \"energy_above_hull\", \"formula_pretty\"]\n",
    "            )\n",
    "\n",
    "            if not thermo_docs:\n",
    "                raise ValueError(f\"在热力学数据库中未找到关于 {formula_to_search} 的材料。\")\n",
    "\n",
    "            # 按稳定性（energy_above_hull）排序，找到最稳定的那个\n",
    "            stable_thermo_docs = sorted(thermo_docs, key=lambda doc: doc.energy_above_hull)\n",
    "            most_stable_thermo_doc = stable_thermo_docs[0]\n",
    "            \n",
    "            # 获取最稳定结构的 material_id，这是我们接下来查询的“钥匙”\n",
    "            stable_material_id = most_stable_thermo_doc.material_id\n",
    "            energy_above_hull = most_stable_thermo_doc.energy_above_hull\n",
    "            pretty_formula = most_stable_thermo_doc.formula_pretty\n",
    "\n",
    "            print(f\"找到最稳定结构 ID: {stable_material_id} (稳定性: {energy_above_hull:.3f} eV/atom)\")\n",
    "\n",
    "            # --- 步骤 2: 使用最稳定的 ID 查询电子结构性质（如带隙） ---\n",
    "            print(\"\\n步骤 2: 使用稳定结构 ID 查询电子结构性质...\")\n",
    "            es_doc = mpr.electronic_structure.search(\n",
    "                material_ids=[stable_material_id],\n",
    "                fields=[\"material_id\", \"band_gap\"]\n",
    "            )\n",
    "            \n",
    "            # es_doc 返回的是列表，我们取第一个\n",
    "            band_gap = es_doc[0].band_gap if es_doc else None\n",
    "            return band_gap\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"查询过程中发生错误: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd28430",
   "metadata": {},
   "source": [
    "#### sciKnowEval的验证器，openEnded问题使用模型验证，其余使用规则直接比对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from threading import Lock\n",
    "from mpy_utils import predict_bandgap_for_structure\n",
    "import json\n",
    "import json_repair\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "MPLock = Lock()\n",
    "def sciKnowEval_rule_verifier(answer_content: str,groundtruth: str, question: str, row=None):\n",
    "    \n",
    "    pattern = r\"\\\\boxed{\\\\text{(.*?)}\"\n",
    "    match = re.search(pattern, answer_content)\n",
    "\n",
    "    # 如果匹配成功，提取捕获的内容\n",
    "    if match:\n",
    "        extracted_answer = match.group(1)\n",
    "      \n",
    "    else:\n",
    "        return False\n",
    "   \n",
    "    if groundtruth.lower() == extracted_answer.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def sciKnowEval_specified_band_gap_material_generation_verifier(answer_content: str,groundtruth: str, question: str) -> bool:\n",
    "    _ ,new_formula=call_huoshan(f\"Please extract the material name from the following question,Your answer should only contain the chemical formula. Do not use subscripts for numbers.\\n\\n content: {answer_content}\",\"doubao\")\n",
    "    new_formula=new_formula.strip()\n",
    "    extra_dict={}\n",
    "    extra_dict[\"new_element\"] = new_formula\n",
    "    with MPLock:\n",
    "        band_gap = call_MP(new_formula)\n",
    "        extra_dict[\"database_band_gap\"] = str(band_gap) if band_gap is not None else \"None\"\n",
    "    # if not band_gap:\n",
    "    if band_gap is None:\n",
    "        json_str= json.dumps({\n",
    "  \"modification_type\": \"string, one of ['substitute', 'remove', 'add', 'exchange']\",\n",
    "  \"new_material_formula\": \"string\",\n",
    "  \"details\": {\n",
    "    \"from_element\": \"string, only for 'substitute' or 'exchange'\",\n",
    "    \"to_element\": \"string, only for 'substitute' or 'exchange'\",\n",
    "    \"element\": \"string, only for 'remove' or 'add'\",\n",
    "    \"coords\": \"list of floats, only for 'add'\"\n",
    "  }\n",
    "})\n",
    "        # print(f\"No material found for the formula: {formula_to_search}\")\n",
    "        _,old_formula =call_huoshan(f\"Please extract the material name from the following question,Your answer should only contain the chemical formula. Do not use subscripts for numbers.\\n\\n content: {question}\",\"doubao\")\n",
    "        old_formula=old_formula.strip()\n",
    "        extra_dict[\"old_element\"] = old_formula\n",
    "        _,parsed_json_str = call_huoshan(f\"\"\"\n",
    "                                    \n",
    "你是一个高精度的信息提取助手。你的任务是分析下面提供的关于材料修改的文本，并从中提取出修改类型、新材料的化学式以及修改细节。\n",
    "\n",
    "严格按照以下 JSON 格式输出，不要包含任何额外的文字、解释或 markdown 的 ```json 标记。公式中的数字不要使用下标形式。\n",
    "\n",
    "{json_str}\n",
    "\n",
    "请处理以下文本：\n",
    "{answer_content}\n",
    "\"\"\",\n",
    "\"doubao\")    \n",
    "        with MPLock:\n",
    "            parsed_json = json_repair.repair_json(parsed_json_str,return_objects=True)\n",
    "            band_gap ,_= predict_bandgap_for_structure(old_formula,parsed_json)\n",
    "            extra_dict[\"model_band_gap\"] = str(band_gap) if band_gap is not None else \"None\"\n",
    "            extra_dict[\"parsed_json\"] = parsed_json\n",
    "    prompt=f\"\"\"\n",
    "        You are an AI verifier specializing in material science. Your task is to determine if the `model_content` is a correct and scientifically valid response to the `question`, considering both the `groundtruth` and the provided `Database Lookup Result`. Output only \"True\" or \"False\".\n",
    "\n",
    "[Context and Inputs Start]\n",
    "Question: {question}\n",
    "Ground Truth: {groundtruth}\n",
    "Model Content: {answer_content}\n",
    "Database Lookup Result for Model's Proposed Material: band_gap: {band_gap}\n",
    "[Context and Inputs End]\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Compare the `model_content` with the `groundtruth` and `Database Lookup Result` in the context of the `question`.\n",
    "\n",
    "The `model_content` is \"True\" if all the following conditions are met:\n",
    "- It proposes a scientifically sound and well-reasoned modification to the starting material, correctly applying one of the specified modification types from the question.\n",
    "- The proposed new material is a logical outcome of this modification, and its rationale clearly explains how this modification is expected to lead towards the target property.\n",
    "- As long as the material given by the model is close to the target in the database, it can be considered as 'True' .\n",
    "\n",
    "The `model_content` is \"False\" if any of the following conditions are met:\n",
    "- The `Database Lookup Result` explicitly contradicts the `model_content`. (e.g., the database shows the proposed chemical formula is unstable, known to not exist, or has established properties that fundamentally oppose the model's claims).\n",
    "- It fails to apply a valid modification type as specified in the question to the correct starting material.\n",
    "- It contains critical scientific flaws in its reasoning or proposed modification.\n",
    "- The proposed new material is not a logical or direct result of the described modification process.\n",
    "- It fundamentally misunderstands the scientific goal or constraints of the question.\n",
    "\n",
    "Strictly output \"True\" or \"False\". Do not add any explanation or other characters.\n",
    "\n",
    "Your output:\n",
    "\"\"\"\n",
    "    _,llm_response=call_huoshan(prompt,\"doubao\")\n",
    "    if llm_response.strip().lower() == \"true\":\n",
    "        return True ,extra_dict\n",
    "    elif llm_response.strip().lower() == \"false\":\n",
    "        return False,extra_dict\n",
    "    else:\n",
    "        print(f\"Unexpected LLM response: {llm_response}\")\n",
    "        return False,extra_dict\n",
    "\n",
    "\n",
    "def sciKnowEval_model_verifier(answer_content: str,groundtruth: str, question: str) -> bool:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI verifier. Your task is to determine if the `model_content` is a correct or acceptable response to the `question`, considering the `groundtruth` as the reference for correctness. Output only \"True\" or \"False\".\n",
    "\n",
    "[Context and Inputs Start]\n",
    "Question: {question}\n",
    "Ground Truth: {groundtruth}\n",
    "Model Content: {answer_content}\n",
    "[Context and Inputs End]\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Compare the `model_content` with the `groundtruth` in the context of the `question`.\n",
    "\n",
    "\"The model_content is \"True\" if it proposes a scientifically sound and well-reasoned modification to the starting material, correctly applying one of the specified modification types from the question, and the proposed new material is a logical outcome of this modification. The rationale should clearly explain how this modification is expected to lead towards the target property. The groundtruth serves as a reference for a potentially valid outcome, but a well-argued alternative solution that also meets the question's constraints and scientific principles is also considered \"True\".\"\n",
    "\n",
    "The model_content is \"False\" if it:\n",
    "Fails to apply a valid modification type as specified in the question to the correct starting material.\n",
    "Contains critical scientific flaws in its reasoning or proposed modification.\n",
    "The proposed new material is not a logical or direct result of the described modification process.\n",
    "Fundamentally misunderstands the scientific goal or constraints of the question.\n",
    "\n",
    "Strictly output \"True\" or \"False\". Do not add any explanation or other characters.\n",
    "\n",
    "Your output is:\n",
    "\"\"\"\n",
    "    if question[:52]==\"You are an expert chemist. Given a chemical equation\" :\n",
    "        prompt = f\"\"\"\n",
    "You are an expert chemist acting as an AI verifier. Your task is to determine if the `model_content` correctly balances the chemical equation presented in the `question`.\n",
    "\n",
    "[Context and Inputs Start]\n",
    "Question: {question}\n",
    "Ground Truth (for reference): {groundtruth}\n",
    "Model Content: {answer_content}\n",
    "[Context and Inputs End]\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "1.  Analyze the chemical equation in the `model_content`.\n",
    "2.  Check if the number of atoms for each element is equal on both the reactant and product sides (i.e., the equation is correctly balanced).\n",
    "3.  The `model_content` is \"True\" if the equation is chemically balanced. Minor formatting differences, such as using \"H2\" instead of \"H₂\" or variations in spacing, are acceptable and should be considered correct. The `ground_truth` is a reference, but the primary criterion is the correctness of the balancing itself.\n",
    "4.  The `model_content` is \"False\" if the equation is not balanced.\n",
    "\n",
    "Strictly output \"True\" or \"False\". Do not add any explanation or other characters.\n",
    "\n",
    "Your output is:\n",
    "\"\"\"\n",
    "\n",
    "    _,llm_response=call_openai(prompt)\n",
    "    if llm_response.strip().lower() == \"true\":\n",
    "        return True\n",
    "    elif llm_response.strip().lower() == \"false\":\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Unexpected LLM response: {llm_response}\")\n",
    "        return False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ecd91",
   "metadata": {},
   "source": [
    "#### 处理sciKnowEval数据：生成generation，调用verifier，整合成符合要求的最终dict格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "from collections import defaultdict\n",
    "\n",
    "list_lock = Lock()\n",
    "\n",
    "num_count=defaultdict(int)\n",
    "def sciKnowEval_process_row(row):\n",
    "    global res_list\n",
    "        \n",
    "    task_type = row[\"type\"]\n",
    "    if task_type == \"mcq-2-choices\" or task_type == \"mcq-4-choices\":\n",
    "        groundtruth = row[\"answerKey\"]\n",
    "    elif task_type == \"open-ended-qa\" or task_type == \"true_or_false\" or task_type == \"filling\":\n",
    "        groundtruth = row[\"answer\"]\n",
    "    \n",
    "    target_task_list = [\n",
    "        # \"mcq-2-choices\", \n",
    "        # \"mcq-4-choices\", \n",
    "        # \"true_or_false\", \n",
    "        \"open-ended-qa\",\n",
    "        \"filling\",\n",
    "        ]\n",
    "    subtask = row[\"details\"][\"subtask\"]\n",
    "    # 限制子任务的类型\n",
    "    if task_type not in target_task_list:\n",
    "        return -1\n",
    "    if subtask == \"specified_band_gap_material_generation\":\n",
    "        return -1\n",
    "    # if subtask !=  \"crystal_structure_and_composition_analysis\":\n",
    "    #     return -1\n",
    "    \n",
    "    # 限制子任务的数量\n",
    "    \n",
    "    # if num_count[task_type] >= 50:\n",
    "    #     return -1\n",
    "\n",
    "    with list_lock:\n",
    "        num_count[task_type] += 1\n",
    "\n",
    "    prompt,format_instruction = sciKnowEval_build_prompt(row)\n",
    "    prompt = prompt.strip()\n",
    "    format_instruction=format_instruction.strip()\n",
    "    \n",
    "    \n",
    "    generations=[]\n",
    "    for i in range(1): # 调用模型的次数，暂定为1\n",
    "        generation={}\n",
    "        generation[\"model\"] = \"DeepSeek-R1\"\n",
    "        reasoning_content, answer_content = call_huoshan(prompt,\"r1\")\n",
    "        reasoning_content= reasoning_content.strip()\n",
    "        answer_content=answer_content.strip()\n",
    "        if task_type == \"mcq-2-choices\" or task_type == \"mcq-4-choices\":\n",
    "            answer_content=answer_content.replace(\"(\", \"\")\n",
    "            answer_content=answer_content.replace(\")\", \"\")\n",
    "        generation[\"reasoning_content\"] = reasoning_content\n",
    "        generation[\"answer_content\"] = answer_content\n",
    "        # Verify the model content\n",
    "        evaluation={}\n",
    "        if subtask == \"specified_band_gap_material_generation\":\n",
    "            # with open(os.devnull, 'w') as devnull:\n",
    "            #     with contextlib.redirect_stdout(devnull):\n",
    "            correctness,extra_tag = sciKnowEval_specified_band_gap_material_generation_verifier(answer_content,groundtruth,prompt)\n",
    "            # 当正常校验不通过时，允许模型回答以一个范围的形式出现  \n",
    "            if not correctness:\n",
    "                prompt,format_instruction = sciKnowEval_build_prompt(row,require_range=True)\n",
    "                prompt = prompt.strip()\n",
    "                format_instruction=format_instruction.strip()\n",
    "                reasoning_content, answer_content = call_huoshan(prompt,\"r1\")\n",
    "                reasoning_content= reasoning_content.strip()\n",
    "                answer_content=answer_content.strip()\n",
    "                generation[\"reasoning_content\"] = reasoning_content\n",
    "                generation[\"answer_content\"] = answer_content\n",
    "                correctness,extra_tag = sciKnowEval_specified_band_gap_material_generation_verifier(answer_content,groundtruth,prompt)\n",
    "\n",
    "        elif task_type == \"open-ended-qa\" or task_type == \"filling\":\n",
    "            correctness = sciKnowEval_model_verifier(answer_content,groundtruth,prompt)\n",
    "        else:\n",
    "            correctness = sciKnowEval_rule_verifier(answer_content,groundtruth,prompt)\n",
    "        evaluation[\"correctness\"] = correctness\n",
    "        evaluation[\"By\"] = \"mengpengyu\"\n",
    "        evaluation[\"Method\"] = \"doubao-1.5-thinking-pro\" if task_type == \"open-ended-qa\" else \"Rule\"\n",
    "        if subtask == \"specified_band_gap_material_generation\":\n",
    "            evaluation[\"extra_tags\"] = [extra_tag]\n",
    "        else :\n",
    "            evaluation[\"extra_tags\"] = []\n",
    "        generation[\"evaluation\"] = evaluation\n",
    "        generations.append(generation)\n",
    "        \n",
    "    if task_type == \"mcq-2-choices\" or task_type == \"mcq-4-choices\":\n",
    "        task_type= \"multiple_choice_single\"\n",
    "    elif task_type == \"open-ended-qa\":\n",
    "        task_type= \"question_answering\" \n",
    "    elif task_type == \"filling\":\n",
    "        task_type= \"fill_in_the_blank\"\n",
    "        \n",
    "    res_dict={}\n",
    "    res_dict[\"id\"] = generate_md5(prompt)\n",
    "    res_dict[\"metadata\"] = row.to_dict()\n",
    "    res_dict[\"source_dataset\"] = \"hicai-zju/SciKnowEval\"\n",
    "    res_dict[\"subject_info\"] = {\n",
    "        \"level_1\": row[\"domain\"],\n",
    "        \"level_2\": None\n",
    "        }  \n",
    "    res_dict[\"task_type\"] = task_type\n",
    "    res_dict[\"languages\"] = \"en\"\n",
    "    res_dict[\"multimedia\"]= []\n",
    "    res_dict[\"question\"] = prompt\n",
    "    res_dict[\"format_instruction\"]=format_instruction\n",
    "    res_dict[\"ground_truth\"] = {\n",
    "            \"final_answer\": groundtruth,\n",
    "            \"unit\": None, \n",
    "            \"solution\": None,\n",
    "            \"extra_tags\": []\n",
    "        }\n",
    "        \n",
    "    res_dict[\"generations\"]=generations\n",
    "    res_dict[\"solve_rate\"]={}\n",
    "    res_dict[\"solve_rate\"][\"DeepSeek-R1\"] = sum(1 for gen in generations if gen[\"evaluation\"][\"correctness\"]) / len(generations)\n",
    "    res_dict[\"prompted_for_correct_answer\"]= False\n",
    "    if task_type == \"multiple_choice_single\" or task_type == \"true_or_false\":\n",
    "        res_dict[\"rl_verifier\"]= \"sciKnowEval_rule_verifier\"\n",
    "    elif subtask == \"specified_band_gap_material_generation\":\n",
    "        res_dict[\"rl_verifier\"]= \"sciKnowEval_specified_band_gap_material_generation_verifier\"\n",
    "    elif task_type == \"question_answering\" or task_type == \"fill_in_the_blank\":\n",
    "        res_dict[\"rl_verifier\"]= \"sciKnowEval_model_verifier\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "\n",
    "    with list_lock:\n",
    "        res_list.append(res_dict)\n",
    "    return res_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6205b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████▉| 69860/70196 [01:01<00:00, 1134.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已达到目标数量 200，中断任务循环。\n",
      "正在关闭线程池，不再等待剩余的慢任务...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import pdb\n",
    "import tqdm\n",
    "res_list = []\n",
    "\n",
    "TARGET_COUNT = 200\n",
    "file_lock= Lock()\n",
    "# 将结果写入JSON文件\n",
    "file_tag=\"openended_filling\"\n",
    "output_file = os.path.join(raw_data_path, f\"SciKnowEval_processed_{file_tag}.jsonl\")\n",
    "executor = ThreadPoolExecutor(max_workers=400)\n",
    "try:\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        # 2. 在 try 块内部提交和处理任务\n",
    "        counter = 0\n",
    "        futures = {executor.submit(sciKnowEval_process_row, row): index for index, row in sciKnowEval_data.iterrows()}\n",
    "        for future in tqdm.tqdm(as_completed(futures), total=len(futures), desc=\"Processing rows\"):\n",
    "            index = futures[future]\n",
    "            try:\n",
    "                res=future.result()\n",
    "                if res != -1:\n",
    "                    counter += 1\n",
    "                    json_str = json.dumps(res, ensure_ascii=False)\n",
    "                    with file_lock:\n",
    "                        try:\n",
    "                            f.write(json_str + \"\\n\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error writing to file {output_file}: {e}\")\n",
    "                            # pdb.set_trace()\n",
    "                    \n",
    "                # if counter % 1000 == 0 and counter != 0:\n",
    "                #     # print(f\"Processed {counter} rows.\")\n",
    "                #     with file_lock, list_lock:\n",
    "                #         try:\n",
    "                #             with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                #                 json.dump(res_list, f, ensure_ascii=False, indent=4)\n",
    "                #         except Exception as e:\n",
    "                #             print(f\"Error writing to file {output_file}: {e}\")\n",
    "                #             # pdb.set_trace()     \n",
    "                \n",
    "                # 当达到目标时中断循环\n",
    "                # if counter >= TARGET_COUNT:\n",
    "                #     print(f\"已达到目标数量 {TARGET_COUNT}，中断任务循环。\")\n",
    "                #     break\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    print(\"正在关闭线程池，不再等待剩余的慢任务...\")\n",
    "    executor.shutdown(wait=False, cancel_futures=True) # 这是唯一被调用的shutdown\n",
    "    # try:\n",
    "    #     print(f\"\\n任务已中断或完成，最终获取了 {len(res_list)} 个结果。\")\n",
    "    #     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    #         json.dump(res_list, f, ensure_ascii=False, indent=4)\n",
    "    #     print(\"文件保存成功！\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error writing to file {output_file}: {e}\")\n",
    "    #     pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845889ed",
   "metadata": {},
   "source": [
    "### 统计r1在各个任务上的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5ff78be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of processed entries: 200\n",
      "Domain: Chemistry\n",
      "\n",
      "  Task Type: balancing_chemical_equation\n",
      "\n",
      "    Subtask: balancing_chemical_equation, Correct: 187, Total: 200, Accuracy: 93.50%\n",
      "\n",
      "    Total: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "raw_data_path= os.path.join(\"raw_data\")\n",
    "data_path= os.path.join(raw_data_path, f\"SciKnowEval_processed_{file_tag}.jsonl\")\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "print(f\"Total number of processed entries: {len(data)}\")\n",
    "true_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "sum_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "for entry in data:\n",
    "    # task_type = entry[\"task_type\"]\n",
    "    domin= entry[\"metadata\"][\"domain\"]\n",
    "    task_type=entry[\"metadata\"][\"details\"][\"task\"]\n",
    "    subtask=entry[\"metadata\"][\"details\"][\"subtask\"]\n",
    "\n",
    "    for generation in entry[\"generations\"]:\n",
    "        if generation[\"evaluation\"][\"correctness\"]:\n",
    "            true_dict[domin][task_type][subtask] += 1\n",
    "            true_dict[domin][task_type][\"taskSum\"]+=1\n",
    "            \n",
    "        sum_dict[domin][task_type][subtask] += 1\n",
    "        sum_dict[domin][task_type][\"taskSum\"]+=1\n",
    "\n",
    "for domain, task_types in sum_dict.items():\n",
    "    print(f\"Domain: {domain}\\n\")\n",
    "    for task_type, subtasks in task_types.items():\n",
    "        print(f\"  Task Type: {task_type}\\n\")\n",
    "        for subtask, count in subtasks.items():\n",
    "            if subtask == \"taskSum\":\n",
    "                print(f\"    Total: {count}\\n\")\n",
    "            else:\n",
    "                true_count = true_dict[domain][task_type][subtask]\n",
    "                print(f\"    Subtask: {subtask}, Correct: {true_count}, Total: {count}, Accuracy: {true_count / count:.2%}\\n\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
