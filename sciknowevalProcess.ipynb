{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e9b805",
   "metadata": {},
   "source": [
    "#### 初始化全局变量，导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1009328",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEED_TARGET_COUNT =True\n",
    "GENERATION_NUM = 1 \n",
    "file_tag=\"chemical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ff416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from model import call_huoshan,call_openai\n",
    "import pandas as pd\n",
    "if \"__file__\" in globals():\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "raw_data_path= os.path.join(\"raw_data\")\n",
    "scienceQA_path = os.path.join(raw_data_path, \"ScienceQA\")\n",
    "sciKnowEval_path = os.path.join(raw_data_path, \"SciKnowEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05edc32a",
   "metadata": {},
   "source": [
    "#### 查看读取sciknoweval数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciKnowEval_path_list=[]\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_biology_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_chemistry_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_material_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_physics_test.jsonl\"))  \n",
    "\n",
    "all_dfs = []\n",
    "for file_path in sciKnowEval_path_list:\n",
    "    temp_df = pd.read_json(file_path, lines=True) # 添加 lines=True\n",
    "    all_dfs.append(temp_df)\n",
    "\n",
    "sciKnowEval_data = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sciKnowEval_data.iloc[52100].choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585b6eb",
   "metadata": {},
   "source": [
    "#### prompt构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223bd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def process_choices(choices):\n",
    "    # 传入是\"text\": [\"15.5 - 17.5%\", \"15 - 17%\", \"14 - 16%\", \"16 - 18%\"], \"label\": [\"A\", \"B\", \"C\", \"D\"]\n",
    "    # 返回的是 \"A: 15.5 - 17.5%, B: 15 - 17%, C: 14 - 16%, D: 16 - 18%\"\n",
    "    texts = choices[\"text\"]\n",
    "    labels = choices[\"label\"]\n",
    "    if len(texts) != len(labels):\n",
    "        raise ValueError(\"Choices and labels must have the same length.\")\n",
    "    formatted_choices_list=[\n",
    "        [f\"({label}) {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "        [f\"{label}: {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "        [f\"{label}. {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "        [f\"{label} - {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "        [f\"{label}) {text.strip()}\\n\" for label, text in zip(labels, texts)],\n",
    "    ]\n",
    "    formatted_choices = random.choice(formatted_choices_list)\n",
    "    return \"\".join(formatted_choices)\n",
    "\n",
    "def sciKnowEval_build_prompt(\n",
    "    row,\n",
    "    require_range=False              \n",
    "                             ):\n",
    "    # prompt = row[\"prompt\"][\"default\"]\n",
    "    task_type= row[\"type\"]\n",
    "    domain = row[\"domain\"]\n",
    "    subtask=row[\"details\"][\"subtask\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = row.get(\"choices\", None)\n",
    "    true_or_false_prompt=[\n",
    "        \"Determine the correctness of this statement, write the correct answer inside a \\\\boxed{} at the end. The wrapped answer will be \\\"Yes\\\" or \\\"No\\\". \",\n",
    "        \"Assess the correctness of the following statement. Your conclusion, which must be either \\\"Yes\\\" or \\\"No\\\", should be placed in \\\\boxed{} at the end. \",\n",
    "        \"Evaluate the truthfulness of the statement. Your final answer should be either \\\"Yes\\\" or \\\"No\\\", enclosed in \\\\boxed{} at the end. \",\n",
    "    ]\n",
    "    choices_prompt=[\n",
    "        \"Answer the question, write the correct answer choice inside a \\\\boxed{} at the end. \",\n",
    "        \"Your final response should conclude with the correct answer choice wrapped in a \\\\boxed{}. \",\n",
    "        \"Please provide the answer to the question, and ensure that your final response includes the correct answer choice wrapped in a \\\\boxed{}. \",\n",
    "    ]\n",
    "    \n",
    "    #task_type的类型有：\n",
    "    #     \"true_or_false\"\n",
    "    #   \n",
    "    #  \"mcq-2-choices\"\n",
    "    # \"open-ended-qa\"\n",
    "\n",
    "    random_index = random.randint(0, 1)\n",
    "    if task_type == \"true_or_false\" :\n",
    "        base_prompt = \"You will be presented with a hypothesis or conjecture. Based on the information provided in a text excerpt or your general knowledge, determine if the hypothesis is true (yes) or false (no). \"\n",
    "        prompt = random.choice(true_or_false_prompt) + \"\\n\\n\"\n",
    "        format_instruction=prompt\n",
    "        problem_list=[\n",
    "            f\"Statement: {question}\\n\\n\",\n",
    "            f\"Hypothesis: {question}\\n\\n\",\n",
    "            f\"Question: {question}\\n\\n\",\n",
    "        ]\n",
    "        problem =base_prompt+ random.choice(problem_list)\n",
    "        if random_index == 0:\n",
    "            prompt= prompt+ problem\n",
    "        else:\n",
    "            prompt = problem + prompt\n",
    "    \n",
    "    elif task_type == \"mcq-4-choices\" or task_type == \"mcq-2-choices\":\n",
    "        \n",
    "        prompt = random.choice(choices_prompt) + \"\\n\\n\"\n",
    "        format_instruction=prompt\n",
    "        if choices is None:\n",
    "            raise ValueError(\"Choices must be provided for mcq-4-choices task type.\")\n",
    "        formatted_choices = process_choices(choices)\n",
    "        problem_list=[\n",
    "            # f\"Question: {qusetion}\",\n",
    "            f\"{question}\",\n",
    "        ]\n",
    "        problem = random.choice(problem_list)\n",
    "        choice_list=[\n",
    "            f\"\\n\\n{formatted_choices}\\n\\n\",\n",
    "        ]\n",
    "        problem += random.choice(choice_list)\n",
    "        if random_index == 0:\n",
    "            prompt = prompt + problem\n",
    "        else:\n",
    "            prompt = problem + prompt\n",
    "    elif task_type == \"open-ended-qa\":\n",
    "        prompt = row[\"prompt\"][\"default\"]+ \"\\n\\n\"\n",
    "        if subtask ==  \"crystal_structure_and_composition_analysis\":\n",
    "            prompt=\"Based on the provided crystallographic data, determine the material's properties as requested in the question and list them clearly.\"\n",
    "        elif subtask == \"molecule_generation\":\n",
    "            prompt = \"You are an expert chemist. Given a brief requirements description for molecule design, your task is to directly design a molecule, output using the SMILES of the molecule. Provide the SMILES of the molecule wrapped in a \\\\boxed{}. \\n\\n\"\n",
    "        \n",
    "        if require_range:\n",
    "            prompt += f\"\"\"\n",
    "            Please provide a plausible band gap range for the new material. The width of the range should be approximately 20% of its central value. For example, for a central value of 4.0 eV, a range like '3.6 - 4.4 eV' would be appropriate.\n",
    "            \"\"\"\n",
    "        format_instruction=prompt\n",
    "        if domain == \"Biology\" and subtask == \"text_summary\":\n",
    "            prompt+=question\n",
    "        else:\n",
    "            prompt+= f\"Question: {question}\"\n",
    "            \n",
    "        if subtask == \"specified_band_gap_material_generation\":\n",
    "            prompt+=\"\"\"Give the predicted chemical formula and bandgap values at the end in this form {\"formula\" : formula, \"bandgap\" :bandgap}\"\"\"\n",
    "\n",
    "    elif task_type == \"filling\":\n",
    "        prompt = \"You are an expert chemist. For the given chemical equation, first provide a step-by-step reasoning on how to balance it. Then, provide the final balanced equation wrapped in a \\\\boxed{}. \\n\\n\"\n",
    "\n",
    "        format_instruction=prompt\n",
    "        prompt += f\"Question: {question}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "    return prompt,format_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1791127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_md5(input_string):\n",
    "    # 创建一个 md5 hash 对象\n",
    "    md5_hash = hashlib.md5()\n",
    "    \n",
    "    # 将输入的字符串转换为字节串（因为 hashlib 需要字节类型的数据）\n",
    "    input_bytes = input_string.encode('utf-8')\n",
    "    \n",
    "    # 更新哈希对象\n",
    "    md5_hash.update(input_bytes)\n",
    "    \n",
    "    # 获取哈希值的十六进制表示\n",
    "    md5_digest = md5_hash.hexdigest()\n",
    "    \n",
    "    return md5_digest \n",
    "\n",
    "# 示例使用\n",
    "input_string = \"Hello, World!\"\n",
    "md5_result = generate_md5(input_string)\n",
    "print(f\"MD5 of '{input_string}': {md5_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_api.client import MPRester\n",
    "import os\n",
    "\n",
    "API_KEY = \"bSpFQg1jCJpFG4CARe0NiSUyXKke56OF\"  # <--- 在这里替换成您的密钥\n",
    "formula_to_search = \"Li2VFe(P2O7)2\"\n",
    "\n",
    "def call_MP(formula_to_search):\n",
    "    try:\n",
    "        with MPRester(api_key=API_KEY) as mpr:\n",
    "\n",
    "            # --- 步骤 1: 查询热力学性质，找到最稳定的材料 ID ---\n",
    "            # 我们先从热力学端点入手，因为稳定性是首要关心的\n",
    "            print(\"\\n步骤 1: 查询热力学性质以确定最稳定的结构...\")\n",
    "            thermo_docs = mpr.materials.thermo.search(\n",
    "                formula=formula_to_search,\n",
    "                fields=[\"material_id\", \"energy_above_hull\", \"formula_pretty\"]\n",
    "            )\n",
    "\n",
    "            if not thermo_docs:\n",
    "                raise ValueError(f\"在热力学数据库中未找到关于 {formula_to_search} 的材料。\")\n",
    "\n",
    "            # 按稳定性（energy_above_hull）排序，找到最稳定的那个\n",
    "            stable_thermo_docs = sorted(thermo_docs, key=lambda doc: doc.energy_above_hull)\n",
    "            most_stable_thermo_doc = stable_thermo_docs[0]\n",
    "            \n",
    "            # 获取最稳定结构的 material_id，这是我们接下来查询的“钥匙”\n",
    "            stable_material_id = most_stable_thermo_doc.material_id\n",
    "            energy_above_hull = most_stable_thermo_doc.energy_above_hull\n",
    "            pretty_formula = most_stable_thermo_doc.formula_pretty\n",
    "\n",
    "            print(f\"找到最稳定结构 ID: {stable_material_id} (稳定性: {energy_above_hull:.3f} eV/atom)\")\n",
    "\n",
    "            # --- 步骤 2: 使用最稳定的 ID 查询电子结构性质（如带隙） ---\n",
    "            print(\"\\n步骤 2: 使用稳定结构 ID 查询电子结构性质...\")\n",
    "            es_doc = mpr.electronic_structure.search(\n",
    "                material_ids=[stable_material_id],\n",
    "                fields=[\"material_id\", \"band_gap\"]\n",
    "            )\n",
    "            \n",
    "            # es_doc 返回的是列表，我们取第一个\n",
    "            band_gap = es_doc[0].band_gap if es_doc else None\n",
    "            return band_gap\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"查询过程中发生错误: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd28430",
   "metadata": {},
   "source": [
    "#### 校验器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from threading import Lock\n",
    "\n",
    "import json\n",
    "import json_repair\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "MPLock = Lock()\n",
    "def sciKnowEval_rule_verifier(answer_content: str,groundtruth: str, question: str, row=None):\n",
    "    \n",
    "    pattern = r\"\\\\boxed{\\\\text{(.*?)}\"  \n",
    "    match = re.search(pattern, answer_content)\n",
    "\n",
    "    # 如果匹配成功，提取捕获的内容\n",
    "    if match:\n",
    "        extracted_answer = match.group(1)\n",
    "      \n",
    "    else:\n",
    "        return False\n",
    "   \n",
    "    if groundtruth.lower() == extracted_answer.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def sciKnowEval_specified_band_gap_material_generation_verifier(answer_content: str,groundtruth: str, question: str) -> bool:\n",
    "    from mpy_utils import predict_bandgap_for_structure\n",
    "    _ ,new_formula=call_huoshan(f\"Please extract the material name from the following question,Your answer should only contain the chemical formula. Do not use subscripts for numbers.\\n\\n content: {answer_content}\",\"doubao\")\n",
    "    new_formula=new_formula.strip()\n",
    "    extra_dict={}\n",
    "    extra_dict[\"new_element\"] = new_formula\n",
    "    with MPLock:\n",
    "        band_gap = call_MP(new_formula)\n",
    "        extra_dict[\"database_band_gap\"] = str(band_gap) if band_gap is not None else \"None\"\n",
    "    # if not band_gap:\n",
    "    if band_gap is None:\n",
    "        json_str= json.dumps({\n",
    "  \"modification_type\": \"string, one of ['substitute', 'remove', 'add', 'exchange']\",\n",
    "  \"new_material_formula\": \"string\",\n",
    "  \"details\": {\n",
    "    \"from_element\": \"string, only for 'substitute' or 'exchange'\",\n",
    "    \"to_element\": \"string, only for 'substitute' or 'exchange'\",\n",
    "    \"element\": \"string, only for 'remove' or 'add'\",\n",
    "    \"coords\": \"list of floats, only for 'add'\"\n",
    "  }\n",
    "})\n",
    "        # print(f\"No material found for the formula: {formula_to_search}\")\n",
    "        _,old_formula =call_huoshan(f\"Please extract the material name from the following question,Your answer should only contain the chemical formula. Do not use subscripts for numbers.\\n\\n content: {question}\",\"doubao\")\n",
    "        old_formula=old_formula.strip()\n",
    "        extra_dict[\"old_element\"] = old_formula\n",
    "        _,parsed_json_str = call_huoshan(f\"\"\"\n",
    "                                    \n",
    "你是一个高精度的信息提取助手。你的任务是分析下面提供的关于材料修改的文本，并从中提取出修改类型、新材料的化学式以及修改细节。\n",
    "\n",
    "严格按照以下 JSON 格式输出，不要包含任何额外的文字、解释或 markdown 的 ```json 标记。公式中的数字不要使用下标形式。\n",
    "\n",
    "{json_str}\n",
    "\n",
    "请处理以下文本：\n",
    "{answer_content}\n",
    "\"\"\",\n",
    "\"doubao\")    \n",
    "        with MPLock:\n",
    "            parsed_json = json_repair.repair_json(parsed_json_str,return_objects=True)\n",
    "            band_gap ,_= predict_bandgap_for_structure(old_formula,parsed_json)\n",
    "            extra_dict[\"model_band_gap\"] = str(band_gap) if band_gap is not None else \"None\"\n",
    "            extra_dict[\"parsed_json\"] = parsed_json\n",
    "    prompt=f\"\"\"\n",
    "You are an AI verifier specializing in material science. Your task is to determine if the model_content is a correct and scientifically valid response, considering the question and the Database Lookup Result. Output only \"True\" or \"False\".\n",
    "\n",
    "[Context and Inputs Start]\n",
    "Question: {question}\n",
    "Model Content: {answer_content}\n",
    "Database Lookup Result for Model's Proposed Material: band_gap: {band_gap}\n",
    "[Context and Inputs End]\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "A response is \"True\" if and only if ALL of the following criteria are met. If ANY criterion is not met, the response is \"False\".\n",
    "\n",
    "1. Numerical Validation (Primary Rule - Must Pass):\n",
    "\n",
    "Let database_gap be the value from the Database Lookup Result.\n",
    "\n",
    "Let model_claimed_gap be the band gap value or range asserted by the model_content.\n",
    "\n",
    "Define an absolute error cap: ABS_CAP = 0.3 eV.\n",
    "\n",
    "Case A: If model_claimed_gap is a single number:\n",
    "\n",
    "Calculate the relative tolerance: relative_tolerance = database_gap * 0.10.\n",
    "Determine the allowed error by taking the stricter of the two: allowed_error = min(relative_tolerance, ABS_CAP).\n",
    "Check if abs(model_claimed_gap - database_gap) <= allowed_error.\n",
    "Case B: If model_claimed_gap is a range (e.g., \"4.5-5.5 eV\"):\n",
    "\n",
    "Let the range be [min_val, max_val].\n",
    "The response is valid only if the database_gap falls within the range, extended by the absolute error cap.\n",
    "Check if (min_val - ABS_CAP) <= database_gap <= (max_val + ABS_CAP).\n",
    "2. Scientific and Logical Validation (Secondary Rules - All Must Pass):\n",
    "\n",
    "(a) Soundness of Strategy: The proposed modification must be a scientifically sound and well-reasoned strategy.\n",
    "\n",
    "(b) Adherence to Instructions: The proposed modification must be one of the types explicitly allowed in the question.\n",
    "\n",
    "(c) Logical Consistency: The new material proposed must be a logical and direct result of the described modification.\n",
    "\n",
    "Strictly output \"True\" or \"False\". Do not add any explanation or other characters.\n",
    "\n",
    "Your output:\n",
    "\"\"\"\n",
    "    _,llm_response=call_huoshan(prompt,\"doubao\")\n",
    "    if llm_response.strip().lower() == \"true\":\n",
    "        return True ,extra_dict\n",
    "    elif llm_response.strip().lower() == \"false\":\n",
    "        return False,extra_dict\n",
    "    else:\n",
    "        print(f\"Unexpected LLM response: {llm_response}\")\n",
    "        return False,extra_dict\n",
    "\n",
    "def sciKnowEval_model_verifier(answer_content: str,groundtruth: str, question: str) -> bool:\n",
    "    pattern = r\"\\\\boxed{(.*)}\"\n",
    "    match = re.search(pattern, answer_content)\n",
    "        # 如果匹配成功，提取捕获的内容\n",
    "    if match:\n",
    "        # print(f\"Extracted content: {match.group(1)}\")\n",
    "        answer_content = match.group(1)  \n",
    "    else:\n",
    "        answer_content = answer_content.strip()\n",
    "    prompt = f\"\"\"\n",
    "You are an AI verifier. Your task is to determine if the `model_content` is a correct or acceptable response to the `question`, considering the `groundtruth` as the reference for correctness. Output only \"True\" or \"False\".\n",
    "\n",
    "[Context and Inputs Start]\n",
    "Question: {question}\n",
    "Ground Truth: {groundtruth}\n",
    "Model Content: {answer_content}\n",
    "[Context and Inputs End]\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Compare the `model_content` with the `groundtruth` in the context of the `question`.\n",
    "\n",
    "\"The model_content is \"True\" if it proposes a scientifically sound and well-reasoned modification to the starting material, correctly applying one of the specified modification types from the question, and the proposed new material is a logical outcome of this modification. The rationale should clearly explain how this modification is expected to lead towards the target property. The groundtruth serves as a reference for a potentially valid outcome, but a well-argued alternative solution that also meets the question's constraints and scientific principles is also considered \"True\".\"\n",
    "\n",
    "The model_content is \"False\" if it:\n",
    "Fails to apply a valid modification type as specified in the question to the correct starting material.\n",
    "Contains critical scientific flaws in its reasoning or proposed modification.\n",
    "The proposed new material is not a logical or direct result of the described modification process.\n",
    "Fundamentally misunderstands the scientific goal or constraints of the question.\n",
    "\n",
    "Strictly output \"True\" or \"False\". Do not add any explanation or other characters.\n",
    "\n",
    "Your output is:\n",
    "\"\"\"\n",
    " \n",
    "\n",
    "    _,llm_response=call_huoshan(prompt,\"doubao\")\n",
    "    if llm_response.strip().lower() == \"true\":\n",
    "        return True\n",
    "    elif llm_response.strip().lower() == \"false\":\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Unexpected LLM response: {llm_response}\")\n",
    "        return False \n",
    "\n",
    "def sciKnowEval_filling_verifier(answer_content: str,groundtruth: str, question: str) -> bool:\n",
    "    pattern = r\"\\\\boxed{(.*)}\"\n",
    "    match = re.search(pattern, answer_content)\n",
    "        # 如果匹配成功，提取捕获的内容\n",
    "    if match:\n",
    "        # print(f\"Extracted content: {match.group(1)}\")\n",
    "        extracted_answer = match.group(1)  \n",
    "    else:\n",
    "        extracted_answer = answer_content.strip()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert chemist acting as an AI verifier. Your task is to determine if the `model_content` correctly balances the chemical equation presented in the `question`.\n",
    "\n",
    "[Context and Inputs Start]\n",
    "Question: {question}\n",
    "Ground Truth (for reference): {groundtruth}\n",
    "Model Content: {extracted_answer}\n",
    "[Context and Inputs End]\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "1.  Analyze the chemical equation in the `model_content`.\n",
    "2.  Check if the number of atoms for each element is equal on both the reactant and product sides (i.e., the equation is correctly balanced).\n",
    "3.  The `model_content` is \"True\" if the equation is chemically balanced. Minor formatting differences, such as using \"H2\" instead of \"H₂\" or variations in spacing, are acceptable and should be considered correct. The `ground_truth` is a reference, but the primary criterion is the correctness of the balancing itself.\n",
    "4.  The `model_content` is \"False\" if the equation is not balanced.\n",
    "\n",
    "Strictly output \"True\" or \"False\". Do not add any explanation or other characters.\n",
    "\n",
    "Your output is:\n",
    "\"\"\"\n",
    "\n",
    "    _,llm_response=call_huoshan(prompt,\"doubao\")\n",
    "    if llm_response.strip().lower() == \"true\":\n",
    "        return True\n",
    "    elif llm_response.strip().lower() == \"false\":\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Unexpected LLM response: {llm_response}\")\n",
    "        return False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ecd91",
   "metadata": {},
   "source": [
    "#### 对单个问题的处理主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "from collections import defaultdict\n",
    "\n",
    "list_lock = Lock()\n",
    "\n",
    "num_count=defaultdict(int)\n",
    "def sciKnowEval_process_row(row):\n",
    "    global res_list\n",
    "        \n",
    "    task_type = row[\"type\"]\n",
    "    if task_type == \"mcq-2-choices\" or task_type == \"mcq-4-choices\":\n",
    "        groundtruth = row[\"answerKey\"]\n",
    "    elif task_type == \"open-ended-qa\" or task_type == \"true_or_false\" or task_type == \"filling\":\n",
    "        groundtruth = row[\"answer\"]\n",
    "    domain = row[\"domain\"]\n",
    "    \n",
    "    target_task_list = [\n",
    "        # \"mcq-2-choices\", \n",
    "        # \"mcq-4-choices\", \n",
    "        # \"true_or_false\", \n",
    "        \"open-ended-qa\",\n",
    "        \"filling\",\n",
    "        ]\n",
    "    subtask = row[\"details\"][\"subtask\"]\n",
    "    # 限制子任务的类型\n",
    "    if task_type not in target_task_list:\n",
    "        return -1\n",
    "    # if not(\n",
    "    #     subtask == \"specified_band_gap_material_generation\" or \n",
    "    #     (domain==\"Biology\" and subtask==\"text_summary\")  \n",
    "    #        ):\n",
    "    #     return -1\n",
    "    # if subtask !=  \"crystal_structure_and_composition_analysis\":\n",
    "    #     return -1\n",
    "    \n",
    "    # 限制子任务的数量\n",
    "    \n",
    "    # if num_count[task_type] >= 50:\n",
    "    #     return -1\n",
    "\n",
    "    with list_lock:\n",
    "        num_count[task_type] += 1\n",
    "\n",
    "    prompt,format_instruction = sciKnowEval_build_prompt(row)\n",
    "    prompt = prompt.strip()\n",
    "    format_instruction=format_instruction.strip()\n",
    "    \n",
    "    \n",
    "    generations=[]\n",
    "    for i in range(GENERATION_NUM): # 调用模型的次数，暂定为1\n",
    "        generation={}\n",
    "        generation[\"model\"] = \"DeepSeek-R1\"\n",
    "        reasoning_content, answer_content = call_huoshan(prompt,\"r1\")\n",
    "        reasoning_content= reasoning_content.strip()\n",
    "        answer_content=answer_content.strip()\n",
    "        if task_type == \"mcq-2-choices\" or task_type == \"mcq-4-choices\":\n",
    "            answer_content=answer_content.replace(\"(\", \"\")\n",
    "            answer_content=answer_content.replace(\")\", \"\")\n",
    "        generation[\"reasoning_content\"] = reasoning_content\n",
    "        generation[\"answer_content\"] = answer_content\n",
    "        # Verify the model content\n",
    "        evaluation={}\n",
    "        if subtask == \"specified_band_gap_material_generation\":\n",
    "            # with open(os.devnull, 'w') as devnull:\n",
    "            #     with contextlib.redirect_stdout(devnull):\n",
    "            correctness,extra_tag = sciKnowEval_specified_band_gap_material_generation_verifier(answer_content,groundtruth,prompt)\n",
    "            # 当正常校验不通过时，允许模型回答以一个范围的形式出现  \n",
    "            if not correctness:\n",
    "                prompt,format_instruction = sciKnowEval_build_prompt(row,require_range=True)\n",
    "                prompt = prompt.strip()\n",
    "                format_instruction=format_instruction.strip()\n",
    "                reasoning_content, answer_content = call_huoshan(prompt,\"r1\")\n",
    "                reasoning_content= reasoning_content.strip()\n",
    "                answer_content=answer_content.strip()\n",
    "                generation[\"reasoning_content\"] = reasoning_content\n",
    "                generation[\"answer_content\"] = answer_content\n",
    "                correctness,extra_tag = sciKnowEval_specified_band_gap_material_generation_verifier(answer_content,groundtruth,prompt)\n",
    "\n",
    "        elif task_type == \"open-ended-qa\" :\n",
    "            \n",
    "            correctness = sciKnowEval_model_verifier(answer_content,groundtruth,prompt)\n",
    "        elif task_type == \"filling\":\n",
    "            correctness = sciKnowEval_filling_verifier(answer_content,groundtruth,prompt)\n",
    "        else:\n",
    "            correctness = sciKnowEval_rule_verifier(answer_content,groundtruth,prompt)\n",
    "        evaluation[\"correctness\"] = correctness\n",
    "        evaluation[\"By\"] = \"mengpengyu\"\n",
    "        evaluation[\"Method\"] = \"doubao-1.6-thinking-pro\" if task_type == \"open-ended-qa\" or task_type==\"filling\" else \"Rule\"\n",
    "        if subtask == \"specified_band_gap_material_generation\":\n",
    "            evaluation[\"extra_tags\"] = [extra_tag]\n",
    "        else :\n",
    "            evaluation[\"extra_tags\"] = []\n",
    "        generation[\"evaluation\"] = evaluation\n",
    "        generations.append(generation)\n",
    "        \n",
    "    if task_type == \"mcq-2-choices\" or task_type == \"mcq-4-choices\":\n",
    "        task_type= \"multiple_choice_single\"\n",
    "    elif task_type == \"open-ended-qa\":\n",
    "        task_type= \"question_answering\"\n",
    "    elif task_type == \"filling\":\n",
    "        task_type= \"question_answering\"\n",
    "        \n",
    "    res_dict={}\n",
    "    res_dict[\"id\"] = generate_md5(prompt)\n",
    "    res_dict[\"metadata\"] = row.to_dict()\n",
    "    res_dict[\"source_dataset\"] = \"hicai-zju/SciKnowEval\"\n",
    "    res_dict[\"subject_info\"] = {\n",
    "        \"level_1\": row[\"domain\"],\n",
    "        \"level_2\": None\n",
    "        }  \n",
    "    res_dict[\"type\"] = task_type\n",
    "    res_dict[\"languages\"] = [\"en\"]\n",
    "    res_dict[\"multimedia\"]= []\n",
    "    res_dict[\"question\"] = prompt\n",
    "    res_dict[\"format_instruction\"]=format_instruction\n",
    "    res_dict[\"ground_truth\"] = {\n",
    "            \"final_answer\": groundtruth,\n",
    "            \"unit\": None, \n",
    "            \"solution\": None,\n",
    "            \"extra_tags\": []\n",
    "        }\n",
    "        \n",
    "    res_dict[\"generations\"]=generations\n",
    "    res_dict[\"solve_rate\"]={}\n",
    "    res_dict[\"solve_rate\"][\"DeepSeek-R1\"] = sum(1 for gen in generations if gen[\"evaluation\"][\"correctness\"]) / len(generations)\n",
    "    res_dict[\"prompted_for_correct_answer\"]= False\n",
    "    if task_type == \"multiple_choice_single\" or task_type == \"true_or_false\":\n",
    "        res_dict[\"rl_verifier\"]= \"sciKnowEval_rule_verifier\"\n",
    "    elif subtask == \"specified_band_gap_material_generation\":\n",
    "        res_dict[\"rl_verifier\"]= \"sciKnowEval_specified_band_gap_material_generation_verifier\"\n",
    "    elif row[\"type\"] == \"filling\":\n",
    "        res_dict[\"rl_verifier\"]= \"sciKnowEval_filling_verifier\"\n",
    "    elif task_type == \"question_answering\" and row[\"type\"]!= \"filling\":\n",
    "        res_dict[\"rl_verifier\"]= \"sciKnowEval_model_verifier\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "\n",
    "    with list_lock:\n",
    "        res_list.append(res_dict)\n",
    "    return res_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be22e26f",
   "metadata": {},
   "source": [
    "#### 多线程调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6205b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import pdb\n",
    "import tqdm\n",
    "res_list = []\n",
    "\n",
    "TARGET_COUNT = 100\n",
    "file_lock= Lock()\n",
    "# 将结果写入JSON文件\n",
    "\n",
    "output_file = os.path.join(raw_data_path, f\"SciKnowEval_processed_{file_tag}.jsonl\")\n",
    "executor = ThreadPoolExecutor(max_workers=400)\n",
    "try:\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        # 2. 在 try 块内部提交和处理任务\n",
    "        counter = 0\n",
    "        futures = {executor.submit(sciKnowEval_process_row, row): index for index, row in sciKnowEval_data.iterrows()}\n",
    "        for future in tqdm.tqdm(as_completed(futures), total=len(futures), desc=\"Processing rows\"):\n",
    "            index = futures[future]\n",
    "            try:\n",
    "                res=future.result()\n",
    "                if res != -1:\n",
    "                    counter += 1\n",
    "                    json_str = json.dumps(res, ensure_ascii=False)\n",
    "                    with file_lock:\n",
    "                        try:\n",
    "                            f.write(json_str + \"\\n\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error writing to file {output_file}: {e}\")\n",
    "                            # pdb.set_trace()\n",
    "                    \n",
    "                # if counter % 1000 == 0 and counter != 0:\n",
    "                #     # print(f\"Processed {counter} rows.\")\n",
    "                #     with file_lock, list_lock:\n",
    "                #         try:\n",
    "                #             with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                #                 json.dump(res_list, f, ensure_ascii=False, indent=4)\n",
    "                #         except Exception as e:\n",
    "                #             print(f\"Error writing to file {output_file}: {e}\")\n",
    "                #             # pdb.set_trace()     \n",
    "                \n",
    "                #### 当达到目标时中断循环\n",
    "                if NEED_TARGET_COUNT and counter >= TARGET_COUNT:\n",
    "                    print(f\"已达到目标数量 {TARGET_COUNT}，中断任务循环。\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    print(\"正在关闭线程池，不再等待剩余的慢任务...\")\n",
    "    executor.shutdown(wait=False, cancel_futures=True) # 这是唯一被调用的shutdown\n",
    "    # try:\n",
    "    #     print(f\"\\n任务已中断或完成，最终获取了 {len(res_list)} 个结果。\")\n",
    "    #     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    #         json.dump(res_list, f, ensure_ascii=False, indent=4)\n",
    "    #     print(\"文件保存成功！\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error writing to file {output_file}: {e}\")\n",
    "    #     pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845889ed",
   "metadata": {},
   "source": [
    "### 统计r1在各个任务上的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "raw_data_path= os.path.join(\"raw_data\")\n",
    "data_path= os.path.join(raw_data_path, f\"SciKnowEval_processed_{file_tag}.jsonl\")\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "print(f\"Total number of processed entries: {len(data)}\")\n",
    "true_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "sum_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "for entry in data:\n",
    "    # task_type = entry[\"task_type\"]\n",
    "    domin= entry[\"metadata\"][\"domain\"]\n",
    "    task_type=entry[\"metadata\"][\"details\"][\"task\"]\n",
    "    subtask=entry[\"metadata\"][\"details\"][\"subtask\"]\n",
    "\n",
    "    for generation in entry[\"generations\"]:\n",
    "        if generation[\"evaluation\"][\"correctness\"]:\n",
    "            true_dict[domin][task_type][subtask] += 1\n",
    "            true_dict[domin][task_type][\"taskSum\"]+=1\n",
    "            \n",
    "        sum_dict[domin][task_type][subtask] += 1\n",
    "        sum_dict[domin][task_type][\"taskSum\"]+=1\n",
    "\n",
    "for domain, task_types in sum_dict.items():\n",
    "    print(f\"Domain: {domain}\\n\")\n",
    "    for task_type, subtasks in task_types.items():\n",
    "        print(f\"  Task Type: {task_type}\\n\")\n",
    "        for subtask, count in subtasks.items():\n",
    "            if subtask == \"taskSum\":\n",
    "                print(f\"    Total: {count}\\n\")\n",
    "            else:\n",
    "                true_count = true_dict[domain][task_type][subtask]\n",
    "                print(f\"    Subtask: {subtask}, Correct: {true_count}, Total: {count}, Accuracy: {true_count / count:.2%}\\n\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
