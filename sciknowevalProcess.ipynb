{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e9b805",
   "metadata": {},
   "source": [
    "### 初始化全局变量，导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "610ff416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from model import call_huoshan,call_openai\n",
    "import pandas as pd\n",
    "if \"__file__\" in globals():\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "raw_data_path= os.path.join(\"raw_data\")\n",
    "scienceQA_path = os.path.join(raw_data_path, \"ScienceQA\")\n",
    "sciKnowEval_path = os.path.join(raw_data_path, \"SciKnowEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673aa773",
   "metadata": {},
   "source": [
    "### 查看并读取sciQA数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f62c49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: /u01/mengpengyu/dataProcess/sciknowevalProcess.ipynb\n",
    "\n",
    "sciQA_path=[]\n",
    "sciQA_path.append(os.path.join(scienceQA_path, \"test-00000-of-00001-f0e719df791966ff.parquet\"))\n",
    "sciQA_path.append(os.path.join(scienceQA_path, \"train-00000-of-00001-1028f23e353fbe3e.parquet\"))\n",
    "sciQA_path.append(os.path.join(scienceQA_path, \"validation-00000-of-00001-6c7328ff6c84284c.parquet\"))\n",
    "\n",
    "all_dfs = []\n",
    "for file_path in sciQA_path:\n",
    "    temp_df = pd.read_parquet(file_path)\n",
    "    all_dfs.append(temp_df)\n",
    "\n",
    "sciQA_data = pd.concat(all_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05edc32a",
   "metadata": {},
   "source": [
    "### 查看读取sciknoweval数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3f7db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciKnowEval_path_list=[]\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_biology_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_chemistry_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_material_test.jsonl\"))\n",
    "sciKnowEval_path_list.append(os.path.join(sciKnowEval_path, \"sciknoweval_physics_test.jsonl\"))  \n",
    "\n",
    "all_dfs = []\n",
    "for file_path in sciKnowEval_path_list:\n",
    "    temp_df = pd.read_json(file_path, lines=True) # 添加 lines=True\n",
    "    all_dfs.append(temp_df)\n",
    "\n",
    "sciKnowEval_data = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585b6eb",
   "metadata": {},
   "source": [
    "#### 根据一个问题，以及不同的文件类型，构建传给模型的最终prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "223bd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_choices(choices):\n",
    "    # 传入是\"text\": [\"15.5 - 17.5%\", \"15 - 17%\", \"14 - 16%\", \"16 - 18%\"], \"label\": [\"A\", \"B\", \"C\", \"D\"]\n",
    "    # 返回的是 \"A: 15.5 - 17.5%, B: 15 - 17%, C: 14 - 16%, D: 16 - 18%\"\n",
    "    texts = choices[\"text\"]\n",
    "    labels = choices[\"label\"]\n",
    "    if len(texts) != len(labels):\n",
    "        raise ValueError(\"Choices and labels must have the same length.\")\n",
    "    formatted_choices = [f\"({label}) {text}\" for label, text in zip(labels, texts)]\n",
    "    return \" \".join(formatted_choices)\n",
    "\n",
    "# test= {\n",
    "#     \"text\": [\"15.5 - 17.5%\", \"15 - 17%\", \"14 - 16%\", \"16 - 18%\"], \n",
    "#     \"label\": [\"A\", \"B\", \"C\", \"D\"]\n",
    "# }\n",
    "# print(process_choices(test))\n",
    "\n",
    "def sciKnowEval_build_prompt(row):\n",
    "    prompt = row[\"prompt\"][\"default\"]\n",
    "    task_type= row[\"type\"]\n",
    "    qusetion = row[\"question\"]\n",
    "    choices = row.get(\"choices\", None)\n",
    "    #task_type的类型有：\n",
    "    #     \"true_or_false\"\n",
    "    #   \n",
    "    #  \"mcq-2-choices\"\n",
    "    # \"open-ended-qa\"\n",
    "    if task_type == \"true_or_false\" or task_type == \"open-ended-qa\":\n",
    "        prompt += f\"Question: {qusetion}\\n\\n\"\n",
    "    \n",
    "    elif task_type == \"mcq-4-choices\" or task_type == \"mcq-2-choices\":\n",
    "        if choices is None:\n",
    "            raise ValueError(\"Choices must be provided for mcq-4-choices task type.\")\n",
    "        formatted_choices = process_choices(choices)\n",
    "        prompt += f\"Question: {qusetion}\"\n",
    "        prompt += f\"\\n\\nChoices: {formatted_choices}\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1791127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD5 of 'Hello, World!': 65a8e27d8879283831b664bd8b7f0ad4\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_md5(input_string):\n",
    "    # 创建一个 md5 hash 对象\n",
    "    md5_hash = hashlib.md5()\n",
    "    \n",
    "    # 将输入的字符串转换为字节串（因为 hashlib 需要字节类型的数据）\n",
    "    input_bytes = input_string.encode('utf-8')\n",
    "    \n",
    "    # 更新哈希对象\n",
    "    md5_hash.update(input_bytes)\n",
    "    \n",
    "    # 获取哈希值的十六进制表示\n",
    "    md5_digest = md5_hash.hexdigest()\n",
    "    \n",
    "    return md5_digest\n",
    "\n",
    "# 示例使用\n",
    "input_string = \"Hello, World!\"\n",
    "md5_result = generate_md5(input_string)\n",
    "print(f\"MD5 of '{input_string}': {md5_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd28430",
   "metadata": {},
   "source": [
    "#### sciKnowEval的验证器，openEnded问题使用模型验证，其余使用规则直接比对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c89d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sciKnowEval_rule_verifier(question, groundtruth, model_content):\n",
    "    if groundtruth == model_content:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def sciKnowEval_model_verifier(question: str, groundtruth: str, model_content: str) -> bool:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI verifier. Your task is to determine if the `model_content` is a correct or acceptable response to the `question`, considering the `groundtruth` as the reference for correctness. Output only \"True\" or \"False\".\n",
    "\n",
    "[Context and Inputs Start]\n",
    "Question: {question}\n",
    "Ground Truth: {groundtruth}\n",
    "Model Content: {model_content}\n",
    "[Context and Inputs End]\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Compare the `model_content` with the `groundtruth` in the context of the `question`.\n",
    "\n",
    "\"The model_content is \"True\" if it proposes a scientifically sound and well-reasoned modification to the starting material, correctly applying one of the specified modification types from the question, and the proposed new material is a logical outcome of this modification. The rationale should clearly explain how this modification is expected to lead towards the target property. The groundtruth serves as a reference for a potentially valid outcome, but a well-argued alternative solution that also meets the question's constraints and scientific principles is also considered \"True\".\"\n",
    "\n",
    "The model_content is \"False\" if it:\n",
    "Fails to apply a valid modification type as specified in the question to the correct starting material.\n",
    "Contains critical scientific flaws in its reasoning or proposed modification.\n",
    "The proposed new material is not a logical or direct result of the described modification process.\n",
    "Fundamentally misunderstands the scientific goal or constraints of the question.\n",
    "\n",
    "Strictly output \"True\" or \"False\". Do not add any explanation or other characters.\n",
    "\n",
    "Your output is:\n",
    "\"\"\"\n",
    "\n",
    "    _,llm_response=call_openai(prompt)\n",
    "    if llm_response.strip().lower() == \"true\":\n",
    "        return True\n",
    "    elif llm_response.strip().lower() == \"false\":\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Unexpected LLM response: {llm_response}\")\n",
    "        return False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ecd91",
   "metadata": {},
   "source": [
    "#### 处理sciKnowEval数据：生成generation，调用verifier，整合成符合要求的最终dict格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c05e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "Lock = Lock()\n",
    "\n",
    "\n",
    "def sciKnowEval_process_row(row):\n",
    "    global res_list\n",
    "    task_type = row[\"type\"]\n",
    "    if task_type == \"mcq-2-choices\" or task_type == \"mcq-4-choices\":\n",
    "        groundtruth = row[\"answerKey\"]\n",
    "    elif task_type == \"open-ended-qa\" or task_type == \"true_or_false\":\n",
    "        groundtruth = row[\"answer\"]\n",
    "\n",
    "    prompt = sciKnowEval_build_prompt(row)\n",
    "    \n",
    "    generations=[]\n",
    "    for i in range(1): # 调用模型的次数，暂定为1\n",
    "        generation={}\n",
    "        generation[\"model\"] = \"DeepSeek-R1\"\n",
    "        reasoning_content, answer_content = call_huoshan(prompt,\"r1\")\n",
    "        answer_content=answer_content.strip()\n",
    "        generation[\"reasoning_content\"] = reasoning_content\n",
    "        generation[\"answer_content\"] = answer_content\n",
    "        # Verify the model content\n",
    "        evaluation={}\n",
    "        if task_type == \"open-ended-qa\":\n",
    "            correctness = sciKnowEval_model_verifier(prompt, groundtruth, answer_content)\n",
    "        else:\n",
    "            correctness = sciKnowEval_rule_verifier(prompt, groundtruth, answer_content)\n",
    "        evaluation[\"correctness\"] = correctness\n",
    "        evaluation[\"By\"] = \"mengpengyu\"\n",
    "        evaluation[\"Method\"] = \"gpt-4o\" if task_type == \"open-ended-qa\" else \"Rule\"\n",
    "        evaluation[\"extra_tags\"] = []\n",
    "        generation[\"evaluation\"] = evaluation\n",
    "        generations.append(generation)\n",
    "        \n",
    "    if task_type == \"mcq-2-choices\" or task_type == \"mcq-4-choices\":\n",
    "        task_type= \"multiple_choice_single\"\n",
    "    elif task_type == \"open-ended-qa\":\n",
    "        task_type= \"question_answering\" \n",
    "        \n",
    "    res_dict={}\n",
    "    res_dict[\"id\"] = generate_md5(prompt)\n",
    "    res_dict[\"metadata\"] = row.to_dict()\n",
    "    res_dict[\"source_dataset\"] = \"hicai-zju/SciKnowEval\"\n",
    "    # res_dict[\"subject_info\"] = row[\"domain\"]   #待定，额外对数据进行打标？\n",
    "    res_dict[\"task_type\"] = task_type\n",
    "    res_dict[\"languages\"] = \"en\"\n",
    "    res_dict[\"multimedia\"]= []\n",
    "    res_dict[\"question\"] = prompt\n",
    "    res_dict[\"ground_truth\"] = {\n",
    "            \"final_answer\": groundtruth,\n",
    "            \"unit\": None, \n",
    "            \"solution\": None,\n",
    "            \"extra_tags\": []\n",
    "        }\n",
    "    res_dict[\"generations\"]=generations\n",
    "    res_dict[\"solve_rate\"] = sum(1 for gen in generations if gen[\"evaluation\"][\"correctness\"]) / len(generations)\n",
    "    res_dict[\"prompted_for_correct_answer\"]= False\n",
    "    with Lock:\n",
    "        res_list.append(res_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6205b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 rows.\n",
      "Processed 20 rows.\n",
      "Processed 30 rows.\n",
      "Processed 40 rows.\n",
      "Processed 50 rows.\n",
      "Processed 60 rows.\n",
      "Processed 70 rows.\n",
      "Processed 80 rows.\n",
      "Processed 90 rows.\n",
      "Processed 100 rows.\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "res_list = []\n",
    "with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    counter = 0\n",
    "    futures = {executor.submit(sciKnowEval_process_row, row): index for index, row in sciKnowEval_data.iloc[25200:25300].iterrows()}\n",
    "    for future in as_completed(futures):\n",
    "        index = futures[future]\n",
    "        try:\n",
    "            future.result()  # 获取结果，确保异常被捕获\n",
    "            counter += 1\n",
    "            if counter % 10 == 0:\n",
    "                print(f\"Processed {counter} rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {e}\")\n",
    "            traceback.print_exc() \n",
    "# 将结果写入JSON文件\n",
    "output_file = os.path.join(raw_data_path, \"SciKnowEval_processed.json\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(res_list, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
